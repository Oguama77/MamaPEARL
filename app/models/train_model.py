# -*- coding: utf-8 -*-
"""preeclamspsia_risk_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wyXIk-EqDwzDDjm6vcY1qQ7DVGB2kz2q
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import resample
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
import xgboost as xgb
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, balanced_accuracy_score, ConfusionMatrixDisplay, auc
import joblib
import warnings
warnings.filterwarnings('ignore')

data = pd.read_excel("preclampsia_data.xlsx")

pd.set_option('display.max_columns', None)

pd.set_option('display.max_rows', None)

data.head(5)

#distribition of people with preeclampsia vs those without
data['TGT'] = data['TGT'].map({0: "Control", 1: "Preeclampsia"})

sns.countplot(x = 'TGT', data = data)
plt.title("Control vs Preeclampsia Cases")
plt.xlabel("Condition")
plt.ylabel("Count")
plt.show()

data['TGT'] = data['TGT'].map({"Control": 0, "Preeclampsia": 1})

#summary statistics by Group
grouped_stats = data.groupby("TGT").describe().T
grouped_stats

"""### Removing Outliers"""

#cleaning the outliers
def remove_outliers(df,columns):
    cleaned_data = df.copy()
    for col in columns:
        Q1 = cleaned_data[col].quantile(0.25)
        Q3 = cleaned_data[col].quantile(0.75)
        IQR = Q3-Q1
        lower = Q1 - 1.5*IQR
        upper = Q3 + 1.5*IQR
        cleaned_data = cleaned_data[(cleaned_data[col]>=lower) & (cleaned_data[col]<=upper)]
    return cleaned_data

features_with_outliers = [
    'AL', 'ALK', 'ALT', 'AST', 'BUN', 'CHOL', 'CRP', 'ESR', 'Gamma',
    'Hb', 'Mg', 'PLT', 'Tbil', 'Urine_AC', 'Urine_PC', 'WBC',
    'SBP', 'DBP', 'PROTEIN'
]

cleaned_data = remove_outliers(data, features_with_outliers)

print(f"Original shape: {data.shape}")
print(f"Cleaned shape: {cleaned_data.shape}")

cleaned_data['TGT'].value_counts()

#visualizations of key variables
key_variables = ['SBP', 'DBP', 'CAL', 'CR_SE', 'Uric', 'BUN', 'T3_1_3', 'T3_1_1', 'AST', 'AL']
for variables in key_variables:
    plt.figure(figsize = (10, 6))
    sns.histplot(data = cleaned_data, x=variables, hue = 'TGT', bins= 30, stat='density', kde=True, element='step', alpha=0.6, common_norm=False,
    multiple='dodge')
    #plt.title(f"Distribution of {variables} by TGT")
    plt.xlabel(variables)
    plt.show()

#trend with gestational age
color_palettes = [
    {0: 'navy', 1: 'crimson'},
    {0: 'teal', 1: 'goldenrod'},
    {0: 'darkgreen', 1: 'indianred'},
    {0: 'slateblue', 1: 'coral'},
    {0: 'darkcyan', 1: 'firebrick'},
    {0: 'red', 1: 'black'},
    {0: 'navy', 1: 'teal'},
    {0: 'blue', 1: 'red'},
    {0: 'green', 1: 'orange'},
    {0: 'purple', 1: "brown"}
]
features = ['SBP', 'DBP', 'CAL', 'CR_SE', 'Uric', 'BUN', 'T3_1_3', 'T3_1_1', 'AST', 'AL']
for i, feature in enumerate(features):
    sns.violinplot(data=cleaned_data, x='PREG_WK', y=feature, hue='TGT', split=True)
    plt.title(f"Trend of {feature} with Gestational Age")
    plt.xlabel("Gestational Age")
    plt.ylabel(feature)
    plt.show()

#weight vs blood pressure(SBP, DBP)

blood_pressure = ['SBP', 'DBP']
color_list = [
    {0: 'red', 1: 'blue'},
    {0: 'yellow', 1: 'teal'}
]

for i, pressure in enumerate(blood_pressure):
    sns.lmplot(data = cleaned_data , x = "T3_1_2",
               y=pressure, hue='TGT', palette=color_list[i % len(color_palettes)], aspect=1.5, ci=None)
    plt.title(f"Trend of {pressure} with Maternal Weight")
    plt.xlabel("Maternal Weight")
    plt.ylabel(pressure)
    plt.show()

#clinical markers with TGT
inflammatory_markers = ['SBP', 'DBP', 'WBC']
palette = [
    {0: 'red', 1: 'blue'},
    {0: 'yellow', 1: 'teal'},
    {0: 'green', 1: 'blue'}
]
for i, marker in enumerate(inflammatory_markers):
    sns.boxplot(data = cleaned_data , x = "TGT",
               y=marker, hue='TGT', palette=palette[i % len(palette)])
    plt.title(f"Distribution of {marker} by TGT")
    plt.xlabel("TGT(0=Control, 1=Preeclampsia)")
    plt.ylabel(marker)
    plt.show()

#inflammatory markers with TGT
inflammatory_markers = ['CRP', 'ESR', 'WBC']
palette = [
    {0: 'red', 1: 'blue'},
    {0: 'yellow', 1: 'teal'},
    {0: 'green', 1: 'blue'}
]
for i, marker in enumerate(inflammatory_markers):
    sns.boxplot(data = cleaned_data , x = "TGT",
               y=marker, hue='TGT', palette=palette[i % len(palette)])
    plt.title(f"Distribution of {marker} by TGT")
    plt.xlabel("TGT(0=Control, 1=Preeclampsia)")
    plt.ylabel(marker)
    plt.show()

!pip install statannotations

# If height is in cm
cleaned_data['BMI'] = cleaned_data['T3_1_2'] / ((cleaned_data['T3_1_1'] / 100) ** 2)

# Variables to plot
from statannotations.Annotator import Annotator
variables = ['BMI', 'SBP', 'DBP']
titles = {
    'BMI': 'BMI Distribution by TGT',
    'SBP': 'Systolic Blood Pressure (SBP) by TGT',
    'DBP': 'Diastolic Blood Pressure (DBP) by TGT'
}

# Custom color palette
palette = {0: 'teal', 1: 'darkorange'}

# Seaborn style without gridlines
sns.set(style="white", palette="pastel")

# Group pairs for annotation (assuming binary TGT: 0 vs 1)
pairs = [(0, 1)]

# Loop through variables and generate annotated violin plots
for var in variables:
    fig, ax = plt.subplots(figsize=(8, 6))

    sns.violinplot(
        data=cleaned_data,
        x="TGT",
        y=var,
        hue="TGT",
        split=True,
        inner="box",
        palette=palette,
        ax=ax
    )

    # Add statistical annotation
    annotator = Annotator(
        ax=ax,
        pairs=pairs,
        data=cleaned_data,
        x="TGT",
        y=var
    )
    annotator.configure(
        test='Mann-Whitney',
        text_format='star',
        loc='outside',
        comparisons_correction=None
    )
    annotator.apply_and_annotate()

    ax.set_title(titles[var], pad=20, y=1.08)
    ax.set_xlabel("TGT (0 = Control, 1 = Preeclampsia)")
    ax.set_ylabel(var)
    ax.legend_.remove()
    ax.grid(False)

    plt.tight_layout()
    plt.show()

# Variables to plot
variables = ['PREG_WK', 'SBP', 'DBP']
titles = {
    'PREG_WK': 'Gestational Age',
    'SBP': 'Systolic Blood Pressure (SBP) by TGT',
    'DBP': 'Diastolic Blood Pressure (DBP) by TGT'
}

# Custom color palette
palette = {0: 'teal', 1: 'darkorange'}

# Seaborn style without gridlines
sns.set(style="white", palette="pastel")

# Group pairs for annotation (assuming binary TGT: 0 vs 1)
pairs = [(0, 1)]

# Loop through variables and generate annotated violin plots
for var in variables:
    fig, ax = plt.subplots(figsize=(8, 6))

    sns.violinplot(
        data=cleaned_data,
        x="TGT",
        y=var,
        hue="TGT",
        split=True,
        inner="box",
        palette=palette,
        ax=ax
    )

    # Add statistical annotation
    annotator = Annotator(
        ax=ax,
        pairs=pairs,
        data=cleaned_data,
        x="TGT",
        y=var
    )
    annotator.configure(
        test='Mann-Whitney',
        text_format='star',
        loc='outside',
        comparisons_correction=None
    )
    annotator.apply_and_annotate()

    ax.set_title(titles[var], pad=20, y=1.08)
    ax.set_xlabel("TGT (0 = Control, 1 = Preeclampsia)")
    ax.set_ylabel(var)
    ax.legend_.remove()
    ax.grid(False)

    plt.tight_layout()
    plt.show()



sns.histplot(cleaned_data['CRP'], bins=50)
plt.title("CRP Distribution")

cleaned_data['CRP'].describe()

"""# Model Building"""

target = 'TGT'
X = cleaned_data.drop(columns=[target])
y = cleaned_data[target]

X_majority  = X[y == 0]
X_minority = X[y == 1]
y_majority = y[y == 0]
y_minority = y[y == 1]

X_majority_downsampled, y_majority_downsampled = resample(X_majority, y_majority, replace=False, n_samples=800, random_state = 42)

X_minority_upsampled, y_minority_upsampled = resample(X_minority, y_minority, replace=True, n_samples=600, random_state=42)

X_balanced = pd.concat([X_majority_downsampled, X_minority_upsampled])
y_balanced = pd.concat([y_majority_downsampled, y_minority_upsampled])

X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""KNeighbors"""

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

y_pred_knn = knn.predict(X_test_scaled)
print(classification_report(y_test, y_pred_knn))

y_proba_knn = knn.predict_proba(X_test_scaled)[:, 1]

precision, recall, thresholds = precision_recall_curve(y_test, y_proba_knn)
pr_auc_knn = auc(recall, precision)

plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_knn:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

cm = confusion_matrix(y_test, y_pred_knn)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

bal_acc_knn = balanced_accuracy_score(y_test, y_pred_knn)
print(f'Balanced Accuracy: {bal_acc_knn:.2f}')



"""Stochastic Gradient Boosting"""

model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
model.fit(X_train_scaled, y_train)

y_pred_gb = model.predict(X_test_scaled)
y_prob_gb = model.predict_proba(X_test_scaled)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_gb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_gb))
#print("\nROC AUC Score:", roc_auc_score(y_test, y_prob))

precision, recall, thresholds = precision_recall_curve(y_test, y_prob_gb)
pr_auc_gb = auc(recall, precision)

plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_gb:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

bal_acc_gb = balanced_accuracy_score(y_test, y_pred_gb)
print(f'Balanced Accuracy: {bal_acc_gb:.2f}')

cm_gb = confusion_matrix(y_test, y_pred_gb)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_gb)
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

"""Random Forest Classifier"""

rf_model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)
rf_model.fit(X_train_scaled, y_train)

y_pred_rf = rf_model.predict(X_test_scaled)
y_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))
#print("\nROC AUC Score:", roc_auc_score(y_test, y_prob_rf))

precision, recall, thresholds = precision_recall_curve(y_test, y_prob_rf)
pr_auc_rf = auc(recall, precision)

plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_rf:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

cm_rf = confusion_matrix(y_test, y_pred_rf)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_rf)
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

bal_acc_rf = balanced_accuracy_score(y_test, y_pred_rf)
print(f'Balanced Accuracy: {bal_acc_knn:.2f}')

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid,
                           scoring='average_precision', cv=5, n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)
best_model = grid_search.best_estimator_

y_pred_proba_grid = best_model.predict_proba(X_test_scaled)[:, 1]
y_pred_grid = best_model.predict(X_test_scaled)

# Confusion matrix and classification metrics
print(confusion_matrix(y_test, y_pred_grid))
print(classification_report(y_test, y_pred_grid))
print("Balanced Accuracy:", balanced_accuracy_score(y_test, y_pred_grid))

# PR-AUC
precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_grid)
pr_auc_score = auc(recall, precision)
plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_score:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()
print("PR-AUC Score:", pr_auc_score)

print("Best Hyperparameters:", grid_search.best_params_)

# Get feature importances
importances = rf_model.feature_importances_
feature_names = X.columns

# Create a DataFrame for visualization
feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)

# Display top 10 features
print(feat_imp_df.head(15))

# Optional: plot
plt.figure(figsize=(10,6))
plt.barh(feat_imp_df['Feature'][:15][::-1], feat_imp_df['Importance'][:15][::-1])
plt.xlabel("Feature Importance")
plt.title("Top 15 Important Features")
plt.tight_layout()
plt.show()

"""Decision Tree Classifier"""

dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)
dt_model.fit(X_train_scaled, y_train)

y_pred_dt = dt_model.predict(X_test_scaled)
y_prob_dt = dt_model.predict_proba(X_test_scaled)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))
#print("\nROC AUC Score:", roc_auc_score(y_test, y_prob_dt))

precision, recall, thresholds = precision_recall_curve(y_test, y_prob_dt)
pr_auc_dt = auc(recall, precision)

plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_dt:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

bal_acc_dt = balanced_accuracy_score(y_test, y_pred_dt)
print(f'Balanced Accuracy: {bal_acc_dt:.2f}')

cm_dt = confusion_matrix(y_test, y_pred_dt)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_dt)
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

"""Logistic Regression"""

lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train_scaled, y_train)

y_pred_lr = lr_model.predict(X_test_scaled)
y_prob_lr= lr_model.predict_proba(X_test_scaled)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lr))
print("\nClassification Report:\n", classification_report(y_test, y_pred_lr))
#print("\nROC AUC Score:", roc_auc_score(y_test, y_prob_lr))

precision, recall, thresholds = precision_recall_curve(y_test, y_prob_lr)
pr_auc_lr = auc(recall, precision)

plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_lr:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

bal_acc_lr = balanced_accuracy_score(y_test, y_pred_lr)
print(f'Balanced Accuracy: {bal_acc_lr:.2f}')

cm_lr = confusion_matrix(y_test, y_pred_lr)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_lr)
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

"""Naive Bayes"""

nb_model = GaussianNB()
nb_model.fit(X_train_scaled, y_train)

y_pred_nb = nb_model.predict(X_test_scaled)
y_prob_nb = nb_model.predict_proba(X_test_scaled)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_nb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_nb))
#print("\nROC AUC Score:", roc_auc_score(y_test, y_prob_nb))

cm_nb = confusion_matrix(y_test, y_pred_nb)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_nb)
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

precision, recall, thresholds = precision_recall_curve(y_test, y_prob_nb)
pr_auc_nb = auc(recall, precision)

plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_nb:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

bal_acc_nb = balanced_accuracy_score(y_test, y_pred_nb)
print(f'Balanced Accuracy: {bal_acc_nb:.2f}')

"""Support Vector Classifier"""

svc_model = SVC(kernel='rbf', probability=True, random_state=42)
svc_model.fit(X_train_scaled, y_train)

y_pred_svc = svc_model.predict(X_test_scaled)
y_prob_svc = svc_model.predict_proba(X_test_scaled)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svc))
print("\nClassification Report:\n", classification_report(y_test, y_pred_svc))
#print("\nROC AUC Score:", roc_auc_score(y_test, y_prob))

precision, recall, thresholds = precision_recall_curve(y_test, y_prob_svc)
pr_auc_svc = auc(recall, precision)

plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_svc:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

bal_acc_svc = balanced_accuracy_score(y_test, y_pred_svc)
print(f'Balanced Accuracy: {bal_acc_svc:.2f}')

"""Logistic Regression with Elastic Net Regularization"""

en_model = LogisticRegression(
    penalty='elasticnet',
    solver='saga',
    l1_ratio=0.5,
    C=1.0,
    max_iter=1000,
    random_state=42
)
en_model.fit(X_train_scaled, y_train)

y_pred_en = model.predict(X_test_scaled)
y_prob_en = model.predict_proba(X_test_scaled)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_en))
print("\nClassification Report:\n", classification_report(y_test, y_pred_en))
#print("\nROC AUC Score:", roc_auc_score(y_test, y_prob))

precision, recall, thresholds = precision_recall_curve(y_test, y_prob_en)
pr_auc_en = auc(recall, precision)

plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_en:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

bal_acc_en = balanced_accuracy_score(y_test, y_pred_en)
print(f'Balanced Accuracy: {bal_acc_en:.2f}')

"""Extreme Gradient Boosting (XGBoost)"""

xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=4,
    subsample=0.8,
    colsample_bytree=0.8,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

xgb_model.fit(X_train_scaled, y_train)

y_pred_xgb = xgb_model.predict(X_test_scaled)
y_prob_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))
#print("\nROC AUC Score:", roc_auc_score(y_test, y_prob))

precision, recall, thresholds = precision_recall_curve(y_test, y_prob_xgb)
pr_auc_xgb = auc(recall, precision)

plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_xgb:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

bal_acc_xgb = balanced_accuracy_score(y_test, y_pred_xgb)
print(f'Balanced Accuracy: {bal_acc_xgb:.2f}')

"""Logistic Regression with Ridge Regularization"""

ridge_model = LogisticRegression(
    penalty='l2',
    solver='lbfgs',
    C=1.0,
    max_iter=1000,
    random_state=42
)
ridge_model.fit(X_train_scaled, y_train)

y_pred_ridge = ridge_model.predict(X_test_scaled)
y_prob_ridge = ridge_model.predict_proba(X_test_scaled)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_ridge))
print("\nClassification Report:\n", classification_report(y_test, y_pred_ridge))
#print("\nROC AUC Score:", roc_auc_score(y_test, y_prob_ridge))

precision, recall, thresholds = precision_recall_curve(y_test, y_prob_ridge)
pr_auc_ridge = auc(recall, precision)

plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_ridge:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

"""Ensemble Voting Classifier"""

from sklearn.ensemble import VotingClassifier

ensemble = VotingClassifier(
    estimators=[
        ('sgb', model),
        ('elasticnet', en_model),
        ('xgb', xgb_model),
        ('rf', rf_model)
    ],
    voting='soft'
)

ensemble.fit(X_train_scaled, y_train)
y_pred_ensemble = ensemble.predict(X_test_scaled)
y_proba_ensemble = ensemble.predict_proba(X_test_scaled)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_ensemble))
print("\nClassification Report:\n", classification_report(y_test, y_pred_ensemble))
#print("\nROC AUC Score:", roc_auc_score(y_test, y_proba_ensemble))

precision, recall, thresholds = precision_recall_curve(y_test, y_proba_ensemble)
pr_auc_ensemble = auc(recall, precision)

plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_ensemble:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

bal_acc_ensemble = balanced_accuracy_score(y_test, y_pred_ensemble)
print(f'Balanced Accuracy: {bal_acc_ensemble:.2f}')

"""Stacking ensemble"""

from sklearn.ensemble import StackingClassifier

# Base learners
estimators = [
    ('sgb', model),
    ('elasticnet', en_model),
    ('rf', rf_model )
]
# Meta-learner (final estimator)
meta_model = xgb_model

stacked_model = StackingClassifier(
    estimators= estimators,
    final_estimator=meta_model,
    cv=5,
    passthrough=True
)

stacked_model.fit(X_train_scaled, y_train)
y_pred_stacked = stacked_model.predict(X_test_scaled)
y_proba_stacked = stacked_model.predict_proba(X_test_scaled)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_stacked))
print("\nClassification Report:\n", classification_report(y_test, y_pred_stacked))
#print("\nROC AUC Score:", roc_auc_score(y_test, y_proba_stacked))

precision, recall, thresholds = precision_recall_curve(y_test, y_proba_stacked)
pr_auc_stacked = auc(recall, precision)

plt.figure()
plt.plot(recall, precision, label=f'PR-AUC = {pr_auc_stacked:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid(True)
plt.show()

bal_acc_stacked = balanced_accuracy_score(y_test, y_pred_stacked)
print(f'Balanced Accuracy: {bal_acc_stacked:.2f}')

plt.figure(figsize=(8, 6))

models = {
    "Random Forest": best_model,
    "XGBoost": xgb_model,
    "Logistic Regression": en_model,
    "KNN": knn,
    "SVC": svc_model,
    "Naive Bayes": nb_model,
    "Decision Tree": dt_model,
    "Stochastic Gradient": model,
}

for name, model in models.items():
    y_proba = model.predict_proba(X_test_scaled)[:, 1]
    precision, recall, threshhold = precision_recall_curve(y_test, y_proba)
    pr_auc = auc(recall, precision)
    plt.plot(recall, precision, label=f"{name} (AUC={pr_auc:.2f})")

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("PR-AUC Curves for All Models")
plt.legend(loc="lower left")
plt.grid(False)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 7))

# Define strong, vivid colors manually
color_palette = {
    "Random Forest": "#1f77b4",    # strong blue
    "XGBoost": "#ff7f0e",          # vivid orange
    "Logistic Regression": "#2ca02c",  # green
    "KNN": "#d62728",              # red
    "SVC": "#9467bd",              # purple
    "Naive Bayes": "#8c564b",      # brown
    "Decision Tree": "#e377c2",    # pink
    "Stochastic Gradient": "#17becf"  # cyan
}

models = {
    "Random Forest": best_model,
    "XGBoost": xgb_model,
    "Logistic Regression": en_model,
    "KNN": knn,
    "SVC": svc_model,
    "Naive Bayes": nb_model,
    "Decision Tree": dt_model,
    "Stochastic Gradient": model,
}

# Plot PR curves
for name, model in models.items():
    y_proba = model.predict_proba(X_test_scaled)[:, 1]
    precision, recall, _ = precision_recall_curve(y_test, y_proba)
    pr_auc = auc(recall, precision)
    plt.plot(recall, precision,
             label=f"{name} (AUC={pr_auc:.2f})",
             linewidth=2.5,
             color=color_palette[name])

# Plot styling
plt.xlabel("Recall", fontsize=14, fontweight='bold')
plt.ylabel("Precision", fontsize=14, fontweight='bold')
plt.title("PR-AUC Curves for All Models", fontsize=16, fontweight='bold', pad=15)
plt.legend(loc="lower left", fontsize=12)
plt.grid(False)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.tight_layout()
plt.show()

